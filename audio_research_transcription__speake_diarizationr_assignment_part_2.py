# -*- coding: utf-8 -*-
"""Audio_research_transcription__speake_diarizationr_assignment_part_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NM9RKLCDWKb6rzuspBlFHpSXVM6CFa_i
"""

#Installing, upgrading and importing whisper and whisperX

!pip install -U openai-whisper
!pip install git+https://github.com/openai/whisper.git
!pip install git+https://github.com/m-bain/whisperx.git --upgrade
!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git
import whisper
import whisperx

# Load the Drive helper and mount

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive/FIN_audio/")

# Setting device and model parameters for the function
device = 'cuda'
batch_size = 16 # reduce if low on GPU mem
compute_type = "float16" # change to "int8" if low on GPU mem (may reduce accuracy)
model = "large-v2"
file = "Abbot_2020Q3.m4a"

# Function for model diarization (Compilation from part 1)

def audio_diarization(model, batch_size, compute_type, device, file):

  # 1. Transcribe the audio

  model_whisper = whisperx.load_model(whisper_arch=model, device=device, compute_type=compute_type)

  audio = whisperx.load_audio(file)

  result_whisper = model_whisper.transcribe(audio, batch_size=batch_size)

  print('Segments before alignment')
  print(result_whisper["segments"]) # before alignment

  print('Length of segments before alignment')
  print(len(result_whisper['segments'])) # length before aligment


  # 2. Align whisper output

  model_align, metadata2 = whisperx.load_align_model(language_code = result_whisper['language'], device = device)

  result_align = whisperx.align(result_whisper["segments"], model_align, metadata2, audio, device, return_char_alignments = False)

  print('Segments after alignment')
  print(result_align["segments"]) # after alignment

  print('Length of segments after alignment')
  print(len(result_align['segments'])) # length before aligment


  # 3. Speaker diarization

  diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device = 'cuda' )

  # add min/max number of speakers if known
  diarize_segments = diarize_model(audio)
  # diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

  print('Length of diarized segments')
  print(len(diarize_segments)) # length before aligment

  # 4. Speaker Assignment

  result_final = whisperx.assign_word_speakers(diarize_segments, result_whisper)

  print('The final result is:')
  print(len(result_final['segments']))
  print(type(result_final["segments"][0]))
  print(type(result_final))
  print(result_final.keys())
  print(result_final["segments"][0].keys())

  return result_final


  #print(diarize_segments)
  #print(result["segments"]) # segments are now assigned speaker IDs
#
  #print(type(diarize_segments))
  #print(diarize_segments['speaker'].unique())
#
  #print(len(result_final["segments"]))

result1 = audio_diarization(model, batch_size, compute_type, device, file)

import pickle

# Serialize the object to a file
with open("abbotQ3.pickle", "wb") as f:
 pickle.dump(result1, f)

# De-serialize the object from the file
with open("abbotQ3.pickle", "rb") as f:
  loaded_object = pickle.load(f)

# Check that the de-serialized object is the same as the original object
print(result1 == loaded_object) # Output: True

# Dummy model template for future improvizations

def audio_diarization2(model, batch_size, compute_type, device, file):

  # 1. Transcribe the audio
  model_whisper = whisperx.load_model(whisper_arch=model, device=device, compute_type=compute_type)

  audio = whisperx.load_audio(file)

  result_whisper = model_whisper.transcribe(audio, batch_size=batch_size)

  print('Segments before alignment')
  print(result_whisper["segments"]) # before alignment

  print('Length of segments before alignment')
  print(len(result_whisper['segments']))


  # 2. Align whisper output

  model_align, metadata2 = whisperx.load_align_model(language_code = result_whisper['language'], device = device)

  result_align = whisperx.align(result_whisper["segments"], model_align, metadata2, audio, device, return_char_alignments = False)

  print('Segments after alignment')
  print(result_align["segments"]) # after alignment

  print('Length of segments after alignment')
  print(len(result_align['segments']))


  # 3. Assign speaker labels

  diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device = 'cuda' )

  # add min/max number of speakers if known
  diarize_segments = diarize_model(audio)
  # diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

  result_final = whisperx.assign_word_speakers(diarize_segments, result_align)

  print('The final result is:')
  print(type(result_final["segments"][0]))
  print(type(result_final))
  print(result_final.keys())
  print(result_final["segments"][0].keys())

  return result_final


  #print(diarize_segments)
  #print(result["segments"]) # segments are now assigned speaker IDs
#
  #print(type(diarize_segments))
  #print(diarize_segments['speaker'].unique())
#
  #print(len(result_final["segments"]))

result2 = audio_diarization2(model, batch_size, compute_type, device, file)

# Creating a dataframe of segments or ease of usage

import pandas as pd

df = pd.DataFrame.from_dict(result1["segments"], orient = 'columns')

#Some basic analysis

df['speaker'].unique() # unique speakers
df[df['speaker'].isna()] # null records

# Clubbing speakers with consecutive records to account for continuity in speech so that entire text of a one speaker at a time can be analyzed.

df['group'] = ((df['speaker'] != df['speaker'].shift(1))).cumsum()

df[['speaker','group']]

#Saving a copy of above dump with groups

df_dump = df

df_dump

def custom_text_aggregator(input):
  return ''.join(list(input))



# Logic for combining text of consecutive speakers for same category. Please ask if logic not understood

df_test = df.groupby('group').agg({'start':'min', 'end':'max', 'speaker': 'max', 'text': lambda x: ''.join(list(x))})


df_test = df_test.reset_index()

df_test

# Checking the overlap issue. Need to find a way to streamline this problem

# Checking texts of speakers in order

df_test.iloc[0]['text']

df_test.iloc[1]['text']

df_test.iloc[2]['text'] # Robert 07

df_test.iloc[3]['text'] #Bob 06

df_test.iloc[4]['text'] # Scott 08

df_test.iloc[5]['text'] # Robbie 04

df_test.iloc[6]['text'] # Robert

df_test[df_test['speaker'] == 'SPEAKER_08']

# Trying to convert into datetime object for further calculations if required. Not complete.

df_test['start'] = pd.to_datetime(df_test['start'], unit='m')
df_test['start'] = pd.to_datetime(df_test['end'], unit='m')

df_test

import pandas as pd

df1 = pd.DataFrame.from_dict(result2["segments"], orient = 'columns')

df1['speaker']

df1[df1['speaker'].isna()]

# Checking for Abbot Q3

result_Q3 = audio_diarization(model, batch_size, compute_type, device, file = "Abbot_2020Q3.m4a")

# Converting into dataframe

import pandas as pd

df3 = pd.DataFrame.from_dict(result_Q3["segments"], orient = 'columns')

df3['speaker'].unique()

df3

# Checking for Abbot Q3

result_Q3_apple = audio_diarization(model, batch_size, compute_type, device, file = "Apple_2020Q3.m4a")

df_Q3_apple = pd.DataFrame.from_dict(result_Q3_apple["segments"], orient = 'columns')

df_Q3_apple['speaker'].unique()

# Checking for Apple Q4

result_Q4_apple = audio_diarization(model, batch_size, compute_type, device, file = "Apple_2020Q4.m4a")

df_Q4_apple = pd.DataFrame.from_dict(result_Q4_apple["segments"], orient = 'columns')