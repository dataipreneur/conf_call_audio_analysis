# -*- coding: utf-8 -*-
"""Audio_research_transcription__speake_diarizationr_assignment_part_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FcN94TYs0y5bX9QNLJCmnCXnmaU4w-B-
"""

#install whisper

!pip install -U openai-whisper

"""This notebook consists of 4 major steps:
1. Transcription of Audio
2. Audio text alignment
3. Speaker Diarization (Idnetification of speakers based on groups)
4. Speaker assignment

1. Transcription of Audio:

For the purpose of experimentation, two models are used for Audio Transcription:
1. Whisper (Limited functionality, number of segments generated was huge)
2. WhisperX ( Improved functionality with methods for speaker diarization pipelines and speaker assignemmt added)

These models were referred via public git repositories available.
Diarization techniques have been fethched from Hugging Face


2.  Audio text alignment:

Improved segmentation occurs which leads to nuanced time stamps of smaller segments. Since we need to segments with diarized segments from step 3, such a segmentations offers much more details and accuarcy.

3.  Speaker Diarization:

Number of speakers are identified in this process. If we know the number of speakers, we can input that value too.


4.  Speaker assigment:

The segments generated in step 2 i.e transcribed audio segments with time stamps are combined with output of step 3 to assign the corresponding speaker.

#############################################################

Important note:

In part 2 of the file, I have made a provision by which all consecutive segments of single speaker in a given instance of time are combined. This helps us to get a single record or transcription of audio when one particular spekaer speaks. For example, speaker 2 sepaks after speaker 1 and then speaker 1 speaks again. As per the provison, we get a combined text of speaker 1 first, then combined text of speaker 2 and again combined text of new speech of speaker 1.



"""

#whsiper and whisperX from git repository
!pip install git+https://github.com/openai/whisper.git
!pip install git+https://github.com/m-bain/whisperx.git --upgrade

# Upgraded version (Please check which one works well)
!pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

# Importing whisper and whisperX libraries
import whisper
import whisperx

#Setting up parameters

device = 'cuda'
batch_size = 16 # reduce if low on GPU mem
compute_type = "float16" # change to "int8" if low on GPU mem (may reduce accuracy)

# Settng up two models for transcription (Experimentation)

# 1. Transcribe with original whisper (batched)

model = whisper.load_model("base")
model2 = whisperx.load_model("large-v2", device, compute_type=compute_type)

# Load the Drive helper and mount

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/My Drive/FIN_audio/")

# Model 1

# Runnning model1 for transcription

# Loading the Quarter2 file Abbot_2020Q2.m4a for testing (result1)

audio = whisper.load_audio("Abbot_2020Q2.m4a")
result = model.transcribe("Abbot_2020Q2.m4a")

# Checking results

result['text']

# Model 2

# Running model2 for transcription

# Loading the Quarter2 file Abbot_2020Q2.m4a for testing (result2)

audio = whisperx.load_audio("Abbot_2020Q2.m4a")
result2 = model2.transcribe(audio, batch_size=batch_size)


print(result2["segments"]) # before alignment

#Printing segments

# Segments include all divisions of text with their time stamp. No speakers are available. Please try printing

# Since whisperX model gives lesser segments, tried using it for testing further

print(len(result['segments']))
print(len(result2['segments']))

# 2. Align whisper output

# Load align, as per my understanding, gives a detailed and more precise segmentaion of the segments generated by transcription model. It seems to refine the output. We can think more on this.

# Experimenting with both results of whiser and whisperX models

# Model1

# Using load_align_model for result1

model_a, metadata = whisperx.load_align_model(language_code = result['language'], device = device)

# Aligning result1

result_align = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments = False)

print(result_align["segments"][0:2]) # after alignment

# Model 2

# Using load_align_model for result2


model_a2, metadata2 = whisperx.load_align_model(language_code = result2['language'], device = device)

# Aligning result2

result_align2 = whisperx.align(result2["segments"], model_a2, metadata2, audio, device, return_char_alignments = False)

print(result_align2["segments"]) # after alignment

# Speaker diarization

# Using diarization model by WhisperX. I did not find a relevant diarization technique. We can customize this process via clustering. WhisperX dizarization need user auth token of higging face. You can create a higging face account and check

# 3. Assign speaker labels

diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device = 'cuda' )

# add min/max number of speakers if known. I did not try this. But might be it can be tried with to see how efficiently it can perform
# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)


diarize_segments = diarize_model(audio)

# Combining diarized segments with results of model1 and model2

# Model 1 (assign_word_speakers)

result_final = whisperx.assign_word_speakers(diarize_segments, result)
print(diarize_segments)
print(result["segments"]) # segments are now assigned speaker IDs

print(type(diarize_segments)) # Its a dataframe
print(diarize_segments['speaker'].unique())# 12 unique speakers

# Model 2 (assign_word_speakers)

result_final2 = whisperx.assign_word_speakers(diarize_segments, result2)
print(diarize_segments)
print(result2["segments"]) # segments are now assigned speaker IDs

print(type(diarize_segments)) # Its a dataframe
print(diarize_segments['speaker'].unique()) # 12 unique speakers

print(len(result_final["segments"])) # Please check the count

print(len(result_final2["segments"])) # Please check the count

#Model1
print(type(result_final["segments"][0]))
print(type(result_final))
print(result_final.keys())
print(result_final["segments"][0].keys())

#Model2
print(type(result_final2["segments"][0]))
print(type(result_final2))
print(result_final2.keys())
print(result_final2["segments"][0].keys())

#Using results of model2 for further analysis. We can check model1 too

# result_final ( output of assign word speakers) is a dictionary with many keys or attributes. Converting dcitionary into dataframe for ease of visualization

import pandas as pd

df = pd.DataFrame.from_dict(result_final2["segments"], orient = 'columns')
#d = pd.DataFrame.from_dict(result_final["segments"][0])
#for i in result_final["segments"][0:2]:
#  d = pd.DataFrame.from_dict(i)

# Trying to check results pf text and speaker.

print(df.iloc[0][['text','speaker']] +
df.iloc[1][['text','speaker']] +
df.iloc[2][['text','speaker']] +
df.iloc[3][['text','speaker']] +
df.iloc[4][['text','speaker']] +
df.iloc[5][['text','speaker']] +
df.iloc[6][['text','speaker']] +
df.iloc[7][['text','speaker']] +
df.iloc[8][['text','speaker']] +
df.iloc[9][['text','speaker']] +
df.iloc[10][['text','speaker']] +
df.iloc[11][['text','speaker']] +
df.iloc[12][['text','speaker']] +
df.iloc[13][['text','speaker']])

# Below is just testing of other audio file Abbot_2020Q1.m4a

audio2 = whisperx.load_audio("Abbot_2020Q1.m4a")
result2_2 = model2.transcribe(audio2, batch_size=batch_size)
print(result2_2["segments"]) # before alignment

print(len(result2_2['segments']))

model_a2_2, metadata2_2 = whisperx.load_align_model(language_code = result2['language'], device = device)

result_align2_2 = whisperx.align(result2_2["segments"], model_a2_2, metadata2_2, audio_2, device, return_char_alignments = False)

print(result_align2_2["segments"]) # after alignment

# 3. Assign speaker labels

diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device = 'cuda' )

# add min/max number of speakers if known
diarize_segments_2 = diarize_model(audio)
# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)

result_final_2 = whisperx.assign_word_speakers(diarize_segments_2, result2_2)
print(diarize_segments_2)

print(diarize_segments_2['speaker'].unique())

print(len(result_final_2["segments"]))

print(type(result_final["segments"][0]))
print(type(result_final))
print(result_final.keys())
print(result_final["segments"][0].keys())

import pandas as pd

df = pd.DataFrame.from_dict(result_final["segments"], orient = 'columns')
#d = pd.DataFrame.from_dict(result_final["segments"][0])
#for i in result_final["segments"][0:2]:
#  d = pd.DataFrame.from_dict(i)

print(df.iloc[0][['text','speaker']] +
df.iloc[1][['text','speaker']] +
df.iloc[2][['text','speaker']] +
df.iloc[3][['text','speaker']] +
df.iloc[4][['text','speaker']] +
df.iloc[5][['text','speaker']] +
df.iloc[6][['text','speaker']] +
df.iloc[7][['text','speaker']] +
df.iloc[8][['text','speaker']] +
df.iloc[9][['text','speaker']] +
df.iloc[10][['text','speaker']] +
df.iloc[11][['text','speaker']] +
df.iloc[12][['text','speaker']] +
df.iloc[13][['text','speaker']])

#Trying another method using pyannotate for speaker diarization. Did not run. Need to investigate

!pip install pyannote.audio

from pyannote.audio import Pipeline

pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization')

from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization",use_auth_token='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' )


# apply the pipeline to an audio file
diarization = pipeline("Abbot_2020Q2.m4a")

# dump the diarization output to disk using RTTM format
with open("audio.rttm", "w") as rttm:
    diarization.write_rttm(rttm)

from pyannote.audio import Pipeline

# Replace "${ACCESS_TOKEN_GOES_HERE}" with your authentication token
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization@2.1",
    use_auth_token="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")

# Replace "${AUDIO_FILE_PATH}" with the path to your audio file
diarization = pipeline("Abbot_2020Q2.m4a")

for segment, _, speaker in diarization.itertracks(yield_label=True):
    print(f'Speaker "{speaker}" - "{segment}"')